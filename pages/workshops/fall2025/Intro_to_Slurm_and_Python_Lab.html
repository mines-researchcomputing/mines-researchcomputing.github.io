

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction to Slurm Lab &mdash; Mines Research Computing  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=63dbbe29" />

  
    <link rel="canonical" href="https://rc-docs.mines.edu/pages/workshops/fall2025/Intro_to_Slurm_and_Python_Lab.html" />
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
<!-- Google Tag Manager -->

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':

new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],

j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=

'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);

})(window,document,'script','dataLayer','GTM-MXJV3MB');</script>

<!-- End Google Tag Manager -->

</head>

<body class="wy-body-for-nav">
<!-- Google Tag Manager (noscript) -->

<noscript><iframe src=https://www.googletagmanager.com/ns.html?id=GTM-MXJV3MB

height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<!-- End Google Tag Manager (noscript) -->


  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html">
            
              <img src="../../../_static/Mines-RC-03.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">General Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview of Research Computing at Mines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guidelines.html">Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computing_options.html">Computing Options at Mines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../consultations.html">Research Computing Consultation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../workshops.html">Workshops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../publications.html">Publications using Mines HPC Systems</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Scientific Visualization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../visualization_guides/2d-prime-plots.html">Graphing using Matplotlib and Creating Interactive Plots and Animations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../visualization_guides/paraview-server-startup.html">Paraview Server Startup and Connection Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../visualization_guides/fluent-remote-visualization.html">Ansys Remote Visualization Client Startup Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../visualization_guides/jupyter-lab-startup.html">Jupyter Lab on HPC platforms at Mines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../visualization_guides/using-vnc-connection-for-fluent-gui-access.html">Remote VNC Setup Connection to Access Linux Desktop Apps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../visualization_guides/ansys-setup-megn423.html">Ansys Fluent Startup Guide for Class MEGN 423 Fall 2025</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/AWS.html">Amazon Web Service (AWS)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Cloud/gcp.html">Google Cloud Platform (GCP)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Management</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../data_storage_management/orebits.html">Orebits Storage Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_storage_management/globus.html">Globus File Transfer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High Performance Computing (HPC)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../systems.html">Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../module_system.html">The Module System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/new_user_guide.html">Getting Started (New User Guide)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/connecting_to_systems.html">Connecting to Mines&#64;HPC Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/python_environments.html">Using Anaconda for python environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/ansys.html">ANSYS Fluent Job Submission Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/matlab.html">Running MATLAB on HPC Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/advanced_slurm_guide.html">Advanced Slurm Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/job_efficiency_xdmod.html">Knowing your job efficiency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/Parallel_Scaling_Guide.html">Parallel Scaling Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/Parallel_Scaling_Guide.html#References">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guides/archived_guides.html">Archived Guides</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Budget Guidance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../budget_guidance/research_computing_resource_guidance.html">Research Computing Guidance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../budget_guidance/guidance-case-study-fenics.html">Case Study: Researcher using Open Source Finite Element Code FEniCS for modeling reacting flows</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Mines Research Computing</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction to Slurm Lab</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/pages/workshops/fall2025/Intro_to_Slurm_and_Python_Lab.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction-to-slurm-lab">
<h1>Introduction to Slurm Lab<a class="headerlink" href="#introduction-to-slurm-lab" title="Link to this heading"></a></h1>
<section id="running-your-first-job">
<h2>Running your first job<a class="headerlink" href="#running-your-first-job" title="Link to this heading"></a></h2>
<p>Access to compute resources on Mines’s HPC platforms in managed through a job scheduler. The job scheduler manages HPC resources by having users send jobs using scripts, asking for resources, what commands to run, and how to run them. The scheduler will launch the script on compute resources when they are available. The script consists of two parts, instructions for the scheduler and the commands that the user wants to run.</p>
<p>A sample script is shown below:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --job-name=&quot;sample&quot;
#SBATCH --nodes=2
#SBATCH --account=&quot;fall25_hpc_workshop&quot; 
#SBATCH --ntasks-per-node=4
#SBATCH --ntasks=8
#SBATCH --time=01:00:00

cd $SCRATCH
ls &gt; myfiles
srun hostname
</pre></div>
</div>
<p>The lines that begin with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> are instructions to the scheduler. In order:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--job-name=&quot;sample&quot;</span></code> - You are naming the job</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--nodes=2</span></code> - You are telling the scheduler that you want to run on two nodes</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--ntasks-per-node=4</span></code> - You want to run four tasks per node for a total of 8 tasks. (The â€”ntasks-per-node line is redundant in this case.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--time=01:00:00</span></code> - You will run no longer than 1 hr.</p></li>
</ol>
<p>The last three lines are normal shell commands:</p>
<ol class="arabic simple" start="7">
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">$SCRATCH</span></code> - You will be put in your scratch directory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">&gt;</span> <span class="pre">myfiles</span></code> - A directory listing will be put in the file myfiles.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">hostname</span></code> - The srun command will launch the program hostname in parallel, in this case 8 copies will be started simultaneously. Note that the ls command is not run in parallel; only a single instance will be launched.</p></li>
</ol>
<p>The script is launched using the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command. By default the standard output and standard error will be put in files with the names <code class="docutils literal notranslate"><span class="pre">slurm-######.out</span></code> and <code class="docutils literal notranslate"><span class="pre">slurm-######.err</span></code> respectively, where <code class="docutils literal notranslate"><span class="pre">######</span></code> is a job number. For example running this script on Wendian produces:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[username@wendian001 ~]$ sbatch dohost
Submitted batch job 363541
After some time:

[username@wendian001 ~]$ ls -lt | head
total 88120
-rw-rw-r--  1 username username        64 Sep 24 16:28 slurm-363541.out
-rw-rw-r--  1 username username      2321 Sep 24 16:28 myfiles
...

[username@wendian001 ~]$ cat slurm-363541.out
c001
c002
c001
c001
c001
c002
c002
c002m
</pre></div>
</div>
</section>
<section id="running-your-first-compiled-job-hello-world">
<h2>Running your first compiled job - Hello World!<a class="headerlink" href="#running-your-first-compiled-job-hello-world" title="Link to this heading"></a></h2>
<p>Now that you can login, navigate the filesystem and understand the basics of the SLURM job scheduler, we are ready to send our first job to the scheduler. For the first example, we are going to show how to send a job using the simple C++ program for <code class="docutils literal notranslate"><span class="pre">Hello</span> <span class="pre">World</span></code>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">	</span><span class="c1">// Source: https://devblogs.microsoft.com/cppblog/cpp-tutorial-hello-world/</span>
<span class="w">	</span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>
<span class="w">	</span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span>
<span class="w">	</span><span class="p">{</span>
<span class="w">	  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Hello, World!&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">	  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">	</span><span class="p">}</span>
</pre></div>
</div>
<p>For this program hello_world.cpp, we can compile it using the system’s C++ compiler <code class="docutils literal notranslate"><span class="pre">g++</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ g++ hello_world.cpp -o hello_world.exe
</pre></div>
</div>
<p>Now, to send this job using a job script, we need to write one using the SLURM preamble definitions. A sample job script <code class="docutils literal notranslate"><span class="pre">run.slurm</span></code> for our Hello Program is below:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --nodes=1 # number of nodes
#SBATCH --ntasks-per-node=1 # number of tasks per node
#SBATCH --ntasks=1 # redundant; total number of tasks: ntasks = nodes * ntasks-per-node
#SBATCH --account=&quot;fall25_hpc_workshop&quot; 
#SBATCH --time=00:00:01 # time in HH:MM:SS
#SBATCH --output output.%j # standard print output labeled with SLURM job id %j
#SBATCH --error error.%j  # standard print error  labeled with SLURM job id %j

echo &quot;Job has started!&quot;
srun hello_world.exe
echo &quot;Job has finished!&quot;
</pre></div>
</div>
<p>We will highlight some aspects of this SLURM script. First, the lines starting with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> are part of a SLURM script’s preamble. There are a multitude of options for the preamble, where you read more about on the SLURM sbatch documentation page here.</p>
<p>The command srun tells SLURM what executable to run (in this case hello_world) and how to run it using all the SLURM preamble options above.</p>
<p>Now to send the job to SLURM we use <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ sbatch run.slurm
</pre></div>
</div>
<p>You should see the following printed to the screen:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Submitted batch job $JOB_ID
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">$JOB_ID</span></code> will look something like <code class="docutils literal notranslate"><span class="pre">5711043</span></code>.</p>
<section id="using-the-module-system-with-slurm-jobs">
<h3>Using the Module System with Slurm Jobs<a class="headerlink" href="#using-the-module-system-with-slurm-jobs" title="Link to this heading"></a></h3>
<p>Software required for high performance computing is complex due to its web of dependencies of libraries and other scientifc software programs. Furthermore, each piece of scientific software may require dependencies that may conflict with existing libraries, or require a different version of a library completely. Hence, a modular system allows one to load and unload dependencies as needed, depending on what is required from the software.</p>
<p>On Linux systems, paths for software and their libraries are determined by setting environmental variables. You can see the settings of all variables by running the command printenv. In most cases, the most important environment variables are <code class="docutils literal notranslate"><span class="pre">PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>. <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is an environment variable that sets a list of directories that can be searched for finding applications. Similarly, <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> defines a list of directories that will be searched to find libraries used by applications. If you enter a command and see <code class="docutils literal notranslate"><span class="pre">command</span> <span class="pre">not</span> <span class="pre">found</span></code> then it is possible the directory containing the application is not in <code class="docutils literal notranslate"><span class="pre">PATH</span></code>; a similar error occurs for <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> when it cannot find a required library. The module system is designed to easily set and unset collections of environment variables. On Mines HPC systems, this is done using the lmod module system. We will briefly describe some common important module commands and also how to use it in context of setting up and submitting a job.</p>
</section>
<section id="important-module-commands">
<h3>Important Module commands<a class="headerlink" href="#important-module-commands" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code> - Lists all available modules in a cascading tree format</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">-r</span> <span class="pre">spider</span> <span class="pre">mpi</span></code> - Lists all modules related to <code class="docutils literal notranslate"><span class="pre">mpi</span></code> in a cascading tree format</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">keyword</span> <span class="pre">gromacs</span></code> - List modules related to the gromacs program</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">apps/gromacs/gcc-ompi-plumed</span></code> - Loads the Gromacs 2021.1 module</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">unload</span> <span class="pre">apps/gromacs/gcc-ompi-plumed</span></code> - Unloads the Gromacs 2021.1 module</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">purge</span></code> - Unloads all currenty loaded modules</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">list</span></code> - Lists all currently loaded modules
Not all applications are accessible by all HPC users. Some codes are commercial and require licensing, and hence PI approval. Some require PI approval for other reasons. If you are unable to load a module, or see permission errors when executing a job, and would like to know how you might obtain access, please submit a help request.</p></li>
</ul>
<p>Some modules have dependencies that need to be manually entered. For example, the gromacs module requires that modules for the compiler and MPI be loaded first. If there is an unsatisfied dependency you will be notified.</p>
</section>
</section>
<section id="running-jobs-with-modules">
<h2>Running jobs with modules<a class="headerlink" href="#running-jobs-with-modules" title="Link to this heading"></a></h2>
<section id="hello-world-using-mpi">
<h3>Hello World! Using MPI<a class="headerlink" href="#hello-world-using-mpi" title="Link to this heading"></a></h3>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">	</span><span class="c1">// Reference:  https://mpi.deino.net/mpi_functions/MPI_Init.html</span>
<span class="w">	</span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;mpi.h&quot;</span>
<span class="w">	</span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;stdio.h&quot;</span>
<span class="w">	</span><span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">std</span><span class="p">;</span>
<span class="w">	</span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">argv</span><span class="p">[])</span>
<span class="w">	</span><span class="p">{</span>
<span class="w">	</span><span class="kt">int</span><span class="w"> </span><span class="n">rank</span><span class="p">,</span><span class="w"> </span><span class="n">np</span><span class="p">;</span>
<span class="w">	</span><span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span><span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span><span class="w"> </span><span class="c1">// initialize MPI</span>
<span class="w">	</span><span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">np</span><span class="p">);</span><span class="w"> </span><span class="c1">// get total number of processors</span>
<span class="w">	</span><span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">rank</span><span class="p">);</span><span class="w"> </span><span class="c1">// get current rank  </span>
<span class="w">	</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Hello World from rank &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; out of &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">np</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; processors!&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
<span class="w">	</span><span class="n">MPI_Finalize</span><span class="p">();</span><span class="w"> </span><span class="c1">// finalize MPI</span>
<span class="w">	</span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">	</span><span class="p">}</span>
</pre></div>
</div>
<p>For the second program, we wanted to demonstrate how to use the system’s module system. This program initializes MPI, defines integer variables to store the number of processors np and the current processor/rank, rank, and then prints Hello World from rank <code class="docutils literal notranslate"><span class="pre">rank</span></code> out of <code class="docutils literal notranslate"><span class="pre">np</span></code> processors, followed by finalizing MPI.</p>
<p>To compile this code, we need to load modules from the module system. First, let’s print out the PATH variable before loading the modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>echo $PATH
</pre></div>
</div>
<p>On Wendian:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ module load compilers/gcc mpi/openmpi/gcc
</pre></div>
</div>
<p>This will load a newer GCC compiler that is required for our OpenMPI library we’re going to use with the code. After loading the modules, print out the PATH variable to see what has changed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>echo $PATH
</pre></div>
</div>
<p>What has changed?</p>
<p>To compile the code, we will use the MPI-wrapped C++ compiler mpicxx:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mpicxx  hello_world.cpp -o hello_world.exe
</pre></div>
</div>
<p>A sample job script (which we will call run.slurm) to use the executable will look like:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --nodes=1 # number of nodes
#SBATCH --ntasks-per-node=12 # number of tasks per node
#SBATCH --ntasks=12 # redundant; total number of tasks: ntasks = nodes * ntasks-per-node
#SBATCH --account=&quot;fall25_hpc_workshop&quot; 
#SBATCH --time=00:00:01 # time in HH:MM:SS
#SBATCH --output output.%j # standard print output labeled with SLURM job id %j
#SBATCH --error error.%j  # standard print error  labeled with SLURM job id %j

# Load the modules used to compile the code in the job script
module load compilers/gcc mpi/openmpi/gcc

echo &quot;Job has started!&quot;
srun hello_world.exe
echo &quot;Job has finished!&quot;
</pre></div>
</div>
<p>As with the last example, we can send the job to the scheduler using the command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ sbatch run.slurm
</pre></div>
</div>
<p>When the job completes, you should see an output file called <code class="docutils literal notranslate"><span class="pre">output.%j</span></code> where <code class="docutils literal notranslate"><span class="pre">%j</span></code> is the job ID. The output for this job should look similar to:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Job has started!
Hello, World! from rank 1 out of 12 processors!
Hello, World! from rank 3 out of 12 processors!
Hello, World! from rank 5 out of 12 processors!
Hello, World! from rank 7 out of 12 processors!
Hello, World! from rank 9 out of 12 processors!
Hello, World! from rank 10 out of 12 processors!
Hello, World! from rank 11 out of 12 processors!
Hello, World! from rank 0 out of 12 processors!
Hello, World! from rank 2 out of 12 processors!
Hello, World! from rank 4 out of 12 processors!
Hello, World! from rank 6 out of 12 processors!
Hello, World! from rank 8 out of 12 processors!
Job has finished!  
</pre></div>
</div>
</section>
<section id="using-python">
<h3>Using Python<a class="headerlink" href="#using-python" title="Link to this heading"></a></h3>
<p>For many cases, Python is a popular scientific computing program language with a large library of modules to choose from. There are so many dependencies that managing python modules through the HPC module system alone would become incredibly cumbersome. As an alternative, we recommend HPC users take control of the python modules they need by creating their own environment using Anaconda. We will briefly go over how to do this below.</p>
<section id="creating-your-own-environment-using-anaconda">
<h4>Creating your own environment using Anaconda<a class="headerlink" href="#creating-your-own-environment-using-anaconda" title="Link to this heading"></a></h4>
<p>First step is to load the desired version Python version:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ module load apps/python3
</pre></div>
</div>
<p>Then create your own python environment using the following command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ conda create --name my_env python=3.13
</pre></div>
</div>
<p>You should see something similiar to the following print to the screen:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Retrieving notices: done
Channels:
 - conda-forge
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: done


==&gt; WARNING: A newer version of conda exists. &lt;==
    current version: 24.11.3
    latest version: 25.7.0

Please update conda by running

    $ conda update -n base -c conda-forge conda



## Package Plan ##

  environment location: /u/pa/sh/ndanes/.conda/envs/my_env

  added / updated specs:
    - python=3.13


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    libgcc-15.1.0              |       h767d61c_5         805 KB  conda-forge
    libgomp-15.1.0             |       h767d61c_5         437 KB  conda-forge
    libuuid-2.41.2             |       he9a06e4_0          36 KB  conda-forge
    openssl-3.5.3              |       h26f9b46_1         3.0 MB  conda-forge
    python-3.13.7              |h2b335a9_100_cp313        32.0 MB  conda-forge
    ------------------------------------------------------------
                                           Total:        36.3 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge 
  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu 
  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 
  ca-certificates    conda-forge/noarch::ca-certificates-2025.8.3-hbd8a1cb_0 
  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.44-h1423503_1 
  libexpat           conda-forge/linux-64::libexpat-2.7.1-hecca717_0 
  libffi             conda-forge/linux-64::libffi-3.4.6-h2dba641_1 
  libgcc             conda-forge/linux-64::libgcc-15.1.0-h767d61c_5 
  libgomp            conda-forge/linux-64::libgomp-15.1.0-h767d61c_5 
  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 
  libmpdec           conda-forge/linux-64::libmpdec-4.0.0-hb9d3cd8_0 
  libsqlite          conda-forge/linux-64::libsqlite-3.50.4-h0c1763c_0 
  libuuid            conda-forge/linux-64::libuuid-2.41.2-he9a06e4_0 
  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 
  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 
  openssl            conda-forge/linux-64::openssl-3.5.3-h26f9b46_1 
  pip                conda-forge/noarch::pip-25.2-pyh145f28c_0 
  python             conda-forge/linux-64::python-3.13.7-h2b335a9_100_cp313 
  python_abi         conda-forge/noarch::python_abi-3.13-8_cp313 
  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 
  tk                 conda-forge/linux-64::tk-8.6.13-noxft_hd72426e_102 
  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 


Proceed ([y]/n)?
</pre></div>
</div>
<p>Once you confirm and the installation completes, you activate the conda environment with the following command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ conda activate my_env
</pre></div>
</div>
<p>You should now see (my_env) to the left on your command line like follows:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(my_env) [username@wendian001 ~]$
</pre></div>
</div>
<p>You can now add packages you need for your conda environment either using pip or the conda commands. For example, we can install the petsc4py package through conda-forge:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(my_env) [username@wendian ~]$ conda install -c conda-forge petsc4py
# You should see something similiar print to the screen:
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Channels:
 - conda-forge
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: done


==&gt; WARNING: A newer version of conda exists. &lt;==
    current version: 24.11.3
    latest version: 25.7.0

Please update conda by running

    $ conda update -n base -c conda-forge conda



## Package Plan ##

  environment location: /u/pa/sh/ndanes/.conda/envs/my_env

  added / updated specs:
    - petsc4py


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    _x86_64-microarch-level-4  | 2_skylake_avx512           8 KB  conda-forge
    attr-2.5.2                 |       h39aace5_0          66 KB  conda-forge
    fftw-3.3.10                |mpi_openmpi_h99e62ba_10         2.0 MB  conda-forge
    hdf5-1.14.6                |mpi_openmpi_h4fb29d0_3         3.8 MB  conda-forge
    hypre-2.32.0               |mpi_openmpi_h398ea61_1         1.9 MB  conda-forge
    keyutils-1.6.3             |       hb9d3cd8_0         131 KB  conda-forge
    libaec-1.1.4               |       h3f801dc_0          36 KB  conda-forge
    libamd-3.3.3               | haaf9dc3_7100102          49 KB  conda-forge
    libblas-3.9.0              | 36_h91f140b_blis          17 KB  conda-forge
    libbtf-2.3.2               | h32481e8_7100102          27 KB  conda-forge
    libcamd-3.3.3              | h32481e8_7100102          46 KB  conda-forge
    libcap-2.76                |       h0b2e76d_0         119 KB  conda-forge
    libcblas-3.9.0             | 36_h3c44731_blis          17 KB  conda-forge
    libccolamd-3.3.4           | h32481e8_7100102          42 KB  conda-forge
    libcholmod-5.3.1           | h59ddab4_7100102         1.1 MB  conda-forge
    libcolamd-3.3.4            | h32481e8_7100102          33 KB  conda-forge
    libcurl-8.14.1             |       h332b0f4_0         439 KB  conda-forge
    libfabric-2.2.0            |       ha770c72_2          14 KB  conda-forge
    libfabric1-2.2.0           |       h3ff6011_2         666 KB  conda-forge
    libgcc-ng-15.1.0           |       h69a702a_5          29 KB  conda-forge
    libgfortran-15.1.0         |       h69a702a_5          28 KB  conda-forge
    libgfortran-ng-15.1.0      |       h69a702a_5          29 KB  conda-forge
    libgfortran5-15.1.0        |       hcea5267_5         1.5 MB  conda-forge
    libhwloc-2.12.1            |default_h7f8ec31_1002         2.3 MB  conda-forge
    libiconv-1.18              |       h3b78370_2         772 KB  conda-forge
    libklu-2.3.5               | hf24d653_7100102         142 KB  conda-forge
    liblapack-3.9.0            |12_hd37a5e2_netlib         2.7 MB  conda-forge
    libnghttp2-1.67.0          |       had1ee68_0         651 KB  conda-forge
    libpmix-5.0.8              |       h4bd6b51_2         714 KB  conda-forge
    libptscotch-7.0.8          | int32_h0de9fd6_2         179 KB  conda-forge
    libscotch-7.0.8            | int32_h16dc488_2         347 KB  conda-forge
    libspqr-4.3.4              | h852d39f_7100102         213 KB  conda-forge
    libstdcxx-15.1.0           |       h8f9b012_5         3.7 MB  conda-forge
    libstdcxx-ng-15.1.0        |       h4852527_5          29 KB  conda-forge
    libsuitesparseconfig-7.10.1| h92d6892_7100102          42 KB  conda-forge
    libsystemd0-257.9          |       h996ca69_0         481 KB  conda-forge
    libudev1-257.9             |       h085a93f_0         141 KB  conda-forge
    libumfpack-6.3.5           | heb53515_7100102         424 KB  conda-forge
    libxml2-2.15.0             |       h26afc86_0          44 KB  conda-forge
    libxml2-16-2.15.0          |       ha9997c6_0         546 KB  conda-forge
    mpi-1.0.1                  |          openmpi           6 KB  conda-forge
    mumps-include-5.8.1        |       h158ef2a_3          19 KB  conda-forge
    mumps-mpi-5.8.1            |       hcd43f66_3         2.6 MB  conda-forge
    numpy-2.3.3                |  py313hf6604e3_0         8.5 MB  conda-forge
    openmpi-5.0.8              |     h2fe1745_107         3.7 MB  conda-forge
    parmetis-4.0.3             |    h02de7a9_1007         270 KB  conda-forge
    petsc-3.23.6               |  real_h5e9295b_0        20.8 MB  conda-forge
    petsc4py-3.23.6            |np2py313h72f38b6_1         1.8 MB  conda-forge
    rdma-core-59.0             |       hecca717_0         1.2 MB  conda-forge
    scalapack-2.2.0            |       h16fb9de_4         1.8 MB  conda-forge
    superlu-7.0.1              |       h8f6e6c4_0         274 KB  conda-forge
    superlu_dist-9.1.0         |       h3349319_0         1.0 MB  conda-forge
    ucc-1.5.1                  |       hb729f83_0         8.4 MB  conda-forge
    ucx-1.19.0                 |       hc93acc0_4         7.4 MB  conda-forge
    ------------------------------------------------------------
                                           Total:        83.0 MB

The following NEW packages will be INSTALLED:

  _x86_64-microarch~ conda-forge/noarch::_x86_64-microarch-level-4-2_skylake_avx512 
  attr               conda-forge/linux-64::attr-2.5.2-h39aace5_0 
  blis               conda-forge/linux-64::blis-0.9.0-h4ab18f5_2 
  c-ares             conda-forge/linux-64::c-ares-1.34.5-hb9d3cd8_0 
  fftw               conda-forge/linux-64::fftw-3.3.10-mpi_openmpi_h99e62ba_10 
  hdf5               conda-forge/linux-64::hdf5-1.14.6-mpi_openmpi_h4fb29d0_3 
  hypre              conda-forge/linux-64::hypre-2.32.0-mpi_openmpi_h398ea61_1 
  icu                conda-forge/linux-64::icu-75.1-he02047a_0 
  keyutils           conda-forge/linux-64::keyutils-1.6.3-hb9d3cd8_0 
  krb5               conda-forge/linux-64::krb5-1.21.3-h659f571_0 
  libaec             conda-forge/linux-64::libaec-1.1.4-h3f801dc_0 
  libamd             conda-forge/linux-64::libamd-3.3.3-haaf9dc3_7100102 
  libblas            conda-forge/linux-64::libblas-3.9.0-36_h91f140b_blis 
  libbtf             conda-forge/linux-64::libbtf-2.3.2-h32481e8_7100102 
  libcamd            conda-forge/linux-64::libcamd-3.3.3-h32481e8_7100102 
  libcap             conda-forge/linux-64::libcap-2.76-h0b2e76d_0 
  libcblas           conda-forge/linux-64::libcblas-3.9.0-36_h3c44731_blis 
  libccolamd         conda-forge/linux-64::libccolamd-3.3.4-h32481e8_7100102 
  libcholmod         conda-forge/linux-64::libcholmod-5.3.1-h59ddab4_7100102 
  libcolamd          conda-forge/linux-64::libcolamd-3.3.4-h32481e8_7100102 
  libcurl            conda-forge/linux-64::libcurl-8.14.1-h332b0f4_0 
  libedit            conda-forge/linux-64::libedit-3.1.20250104-pl5321h7949ede_0 
  libev              conda-forge/linux-64::libev-4.33-hd590300_2 
  libevent           conda-forge/linux-64::libevent-2.1.12-hf998b51_1 
  libfabric          conda-forge/linux-64::libfabric-2.2.0-ha770c72_2 
  libfabric1         conda-forge/linux-64::libfabric1-2.2.0-h3ff6011_2 
  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.1.0-h69a702a_5 
  libgcrypt-lib      conda-forge/linux-64::libgcrypt-lib-1.11.1-hb9d3cd8_0 
  libgfortran        conda-forge/linux-64::libgfortran-15.1.0-h69a702a_5 
  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-15.1.0-h69a702a_5 
  libgfortran5       conda-forge/linux-64::libgfortran5-15.1.0-hcea5267_5 
  libgpg-error       conda-forge/linux-64::libgpg-error-1.55-h3f2d84a_0 
  libhwloc           conda-forge/linux-64::libhwloc-2.12.1-default_h7f8ec31_1002 
  libiconv           conda-forge/linux-64::libiconv-1.18-h3b78370_2 
  libklu             conda-forge/linux-64::libklu-2.3.5-hf24d653_7100102 
  liblapack          conda-forge/linux-64::liblapack-3.9.0-12_hd37a5e2_netlib 
  libnghttp2         conda-forge/linux-64::libnghttp2-1.67.0-had1ee68_0 
  libnl              conda-forge/linux-64::libnl-3.11.0-hb9d3cd8_0 
  libpmix            conda-forge/linux-64::libpmix-5.0.8-h4bd6b51_2 
  libptscotch        conda-forge/linux-64::libptscotch-7.0.8-int32_h0de9fd6_2 
  libscotch          conda-forge/linux-64::libscotch-7.0.8-int32_h16dc488_2 
  libspqr            conda-forge/linux-64::libspqr-4.3.4-h852d39f_7100102 
  libssh2            conda-forge/linux-64::libssh2-1.11.1-hcf80075_0 
  libstdcxx          conda-forge/linux-64::libstdcxx-15.1.0-h8f9b012_5 
  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-15.1.0-h4852527_5 
  libsuitesparsecon~ conda-forge/linux-64::libsuitesparseconfig-7.10.1-h92d6892_7100102 
  libsystemd0        conda-forge/linux-64::libsystemd0-257.9-h996ca69_0 
  libudev1           conda-forge/linux-64::libudev1-257.9-h085a93f_0 
  libumfpack         conda-forge/linux-64::libumfpack-6.3.5-heb53515_7100102 
  libxml2            conda-forge/linux-64::libxml2-2.15.0-h26afc86_0 
  libxml2-16         conda-forge/linux-64::libxml2-16-2.15.0-ha9997c6_0 
  lz4-c              conda-forge/linux-64::lz4-c-1.10.0-h5888daf_1 
  metis              conda-forge/linux-64::metis-5.1.0-hd0bcaf9_1007 
  mpi                conda-forge/noarch::mpi-1.0.1-openmpi 
  mumps-include      conda-forge/linux-64::mumps-include-5.8.1-h158ef2a_3 
  mumps-mpi          conda-forge/linux-64::mumps-mpi-5.8.1-hcd43f66_3 
  numpy              conda-forge/linux-64::numpy-2.3.3-py313hf6604e3_0 
  openmpi            conda-forge/linux-64::openmpi-5.0.8-h2fe1745_107 
  parmetis           conda-forge/linux-64::parmetis-4.0.3-h02de7a9_1007 
  petsc              conda-forge/linux-64::petsc-3.23.6-real_h5e9295b_0 
  petsc4py           conda-forge/4linux-64::petsc4py-3.23.6-np2py313h72f38b6_1 
  rdma-core          conda-forge/linux-64::rdma-core-59.0-hecca717_0 
  scalapack          conda-forge/linux-64::scalapack-2.2.0-h16fb9de_4 
  superlu            conda-forge/linux-64::superlu-7.0.1-h8f6e6c4_0 
  superlu_dist       conda-forge/linux-64::superlu_dist-9.1.0-h3349319_0 
  ucc                conda-forge/linux-64::ucc-1.5.1-hb729f83_0 
  ucx                conda-forge/linux-64::ucx-1.19.0-hc93acc0_4 
  yaml               conda-forge/linux-64::yaml-0.2.5-h280c20c_3 
  zstd               conda-forge/linux-64::zstd-1.5.7-hb8e6e7a_2 

</pre></div>
</div>
<p>Confirm by typing ‘y’ and pressing enter. You now should have your own Python environment with petsc4py!</p>
</section>
<section id="deactivating-your-conda-environment">
<h4>Deactivating your conda environment<a class="headerlink" href="#deactivating-your-conda-environment" title="Link to this heading"></a></h4>
<p>Once you are done with your environment, you can disable it by issuing the command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ conda deactivate
</pre></div>
</div>
</section>
</section>
</section>
<section id="organizing-your-jobs-using-environment-variables-between-your-home-and-scratch-directory">
<h2>Organizing your jobs using environment variables between your home and scratch directory<a class="headerlink" href="#organizing-your-jobs-using-environment-variables-between-your-home-and-scratch-directory" title="Link to this heading"></a></h2>
<p>The easiest way to organize SLURM jobs is to use the provided slurm job ID that is automatically assigned when your job is submitted. This job ID is conveniently stored in an environment variable called <code class="docutils literal notranslate"><span class="pre">SLURM_JOBID</span></code>. In many HPC environments, your scratch directory is faster for input/output, but it usually not backed up, whereas your home directory is. For the job example below, we will re-use our “Hello World using MPI!” example above, but automatically move the job workload to scratch, including its output.</p>
<p>Calling the file <code class="docutils literal notranslate"><span class="pre">submit_scratch_1.sh</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --nodes=1 # number of nodes
#SBATCH --ntasks-per-node=12 # number of tasks per node
#SBATCH --ntasks=12 # redundant; total number of tasks: ntasks = nodes * ntasks-per-node
#SBATCH --account=&quot;fall25_hpc_workshop&quot; 
#SBATCH --time=00:00:01 # time in HH:MM:SS
# notice we removed the output and error file comments of SBATCH

# Load the modules used to compile the code in the job script
module load compilers/gcc mpi/openmpi/gcc

echo &quot;Job has started!&quot;

echo &quot;Moving job output to scratch&quot;
mkdir -p ${SCRATCH}/jobs/${SLURM_JOBID}
cp hello_world.exe ${SCRATCH}/jobs/${SLURM_JOBID}
cd ${SCRATCH}/jobs/${SLURM_JOBID}
srun hello_world.exe 1&gt; output.${SLURM_JOBID} 2&gt; error.${SLURM_JOBID} # 1&gt; refers to stdout, 2&gt; refers to stderr
echo &quot;Job has finished!&quot;
</pre></div>
</div>
<p>After the file is saved, try submitting it</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>sbatch submit_scratch_1.sh
</pre></div>
</div>
<p>You should still see a <code class="docutils literal notranslate"><span class="pre">slurm-&lt;jobid&gt;.out</span></code> file in your directory. But now check your scratch directory under <code class="docutils literal notranslate"><span class="pre">'jobs</span></code></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>cd $SCRATCH/jobs
cd &lt;jobid&gt;
</pre></div>
</div>
<p>Now look at the contents of that folder, you should see something like:
$ ls
hello_world.exe  output.<jobid></p>
<p>So now we have a reference copy of the output and error (note that the error file won’t exist it the job runs successfully).</p>
<section id="an-alternative-setup-using-slurm-submit-dir">
<h3>An alternative setup using <code class="docutils literal notranslate"><span class="pre">SLURM_SUBMIT_DIR</span></code><a class="headerlink" href="#an-alternative-setup-using-slurm-submit-dir" title="Link to this heading"></a></h3>
<p>If we don’t want to make a copy of <code class="docutils literal notranslate"><span class="pre">hello_world.exe</span></code> to the output directory, we can make use of the <code class="docutils literal notranslate"><span class="pre">SLURM_SUBMIT_DIR</span></code> environment variable to refer to the original folder we submitted the job from. We will also show that you can redirect the output and error files directly with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> below. To do so, we first need to print out our <code class="docutils literal notranslate"><span class="pre">SCRATCH</span></code> path:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ echo ${SCRATCH}
/scratch/&lt;your username&gt;
</pre></div>
</div>
<p>We’ll call this file <code class="docutils literal notranslate"><span class="pre">submit_scratch_2.sh</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --nodes=1 # number of nodes
#SBATCH --ntasks-per-node=12 # number of tasks per node
#SBATCH --ntasks=12 # redundant; total number of tasks: ntasks = nodes * ntasks-per-node
#SBATCH --account=&quot;fall25_hpc_workshop&quot; 
#SBATCH --time=00:00:01 # time in HH:MM:SS
#SBATCH --output /scratch/&lt;your username&gt;/jobs/%j/output.%j # standard print output labeled with SLURM job id %j
#SBATCH --error /scratch/&lt;your username&gt;/jobs/%j/error.%j  # standard print error  labeled with SLURM job id %j

# Load the modules used to compile the code in the job script
module load compilers/gcc mpi/openmpi/gcc

echo &quot;Job has started!&quot;

echo &quot;Moving job output to scratch&quot;
mkdir -p ${SCRATCH}/jobs/${SLURM_JOBID}
cd ${SCRATCH}/jobs/${SLURM_JOBID}
srun ${SLURM_SUBMIT_DIR}/hello_world.exe
echo &quot;Job has finished!&quot;
</pre></div>
</div>
<p>After the file is saved, try submitting it</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>sbatch submit_scratch_2.sh
</pre></div>
</div>
<p>And compare the differences between it and <code class="docutils literal notranslate"><span class="pre">submit_scratch_1.sh</span></code>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
<jinja2.runtime.BlockReference object at 0x7fd3fdcffd40>
  <footer role="contentinfo">
  <div class="column left">
   <h3><br><strong>Mines Research Computing</strong><hr></h3>

<big><strong><a href="https://rc.mines.edu"><u>Research Computing (RC)</u></a></strong><br>
          Colorado School of Mines<br>
          1500 Illinois St., Golden, CO 80401<br>
          303-273-3000 / 800-446-9488 </big>
  </div>
 
   <div class="column middle">
<h3><br><strong>Quick Links</strong><hr></h3>
<big>
  <ul> 
    <li><a href="https://outlook.office365.com/owa/calendar/ResearchComputing@mines0.onmicrosoft.com/bookings/"><u>Schedule a Meeting</u></a></li>
    <li><a href="https://helpcenter.mines.edu/TDClient/1946/Portal/Requests/TicketRequests/NewForm?ID=4GCQlvW5OYk_&RequestorType=Service"><u>Submit an RC ticket</u></a></li>
    <li><a href="https://helpcenter.mines.edu"><u>Mines Help Center</u></a></li>
  </ul>
</big>
<hr>
<big>
  <ul>  
    <li><a href="https://ask.cyberinfrastructure.org/"><u>Ask.Ci</u></a> (External)</li>
    <li><a href="https://it.mines.edu/"><u>Mines IT</u></a></li>
    <li><a href="https://library.mines.edu/"><u>Mines Library</u></a></li>
</ul>
</big>




</div>
  <div class="column right">
<h3><br><strong>About RC</strong><hr></h3>
<p>Mines IT Research Computing (RC) group works to identify research needs across the university and aims to provide comprehensive and innovative services and infrastructure to further research and meet our vision set by the Mines@150 strategic plan. <br>
</p><hr>

<p></p>
</div>


<div>
  <div><p style="text-align: center"><big><a href="https://www.mines.edu" target="_blank" ">Mines Home</a> <strong>|</strong> <a href="https://www.mines.edu/accessibility" target="_blank" ">Accessibility</a> <strong>|</strong> <a href="https://www.mines.edu/privacy" target="_blank" ">Privacy</a> <strong>|</strong> © 2022-24 Colorado School of Mines</big></p></div></div></div>  
      </div>
    
  </footer>

  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
   


</body>
</html>